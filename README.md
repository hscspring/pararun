# Pararunï¼šä¸ºè„šæœ¬ä¸Žé•¿æœŸä»»åŠ¡è€Œç”Ÿçš„å¹¶å‘æ‰§è¡Œåº“

**Pararun** æ˜¯ä¸€ä¸ªé¢å‘å·¥ç¨‹å®žè·µçš„ Python å¹¶å‘æ‰§è¡Œåº“ï¼Œç”¨äºŽ**ç¨³å®šã€å¯æ¢å¤åœ°è¿è¡Œå¤§é‡ä»»åŠ¡**ã€‚
å®ƒä¸“æ³¨è§£å†³ä¸€ä¸ªåœ¨çœŸå®žé¡¹ç›®ä¸­åå¤å‡ºçŽ°ã€å´é•¿æœŸè¢«ä½Žä¼°çš„é—®é¢˜ï¼š

> **å¦‚ä½•åœ¨ä¸é‡å†™ä»£ç ã€ä¸å¼•å…¥å¤æ‚æ¡†æž¶çš„å‰æä¸‹ï¼ŒæŠŠâ€œè·‘ä¸€æ¬¡å°±å¯èƒ½å´©â€çš„è„šæœ¬ï¼Œå˜æˆå¯ä¸­æ–­ã€å¯æ¢å¤ã€å¯é•¿æœŸè¿è¡Œçš„ä»»åŠ¡ç³»ç»Ÿï¼Ÿ**

Pararun ä¸æ˜¯ä¸ºäº†è¿½æ±‚æžè‡´åžåï¼Œè€Œæ˜¯ä¸ºäº†**è®©ä»»åŠ¡â€œè·‘å¾—ä¹…ã€è·‘å¾—ç¨³ã€è·‘å¾—çœå¿ƒâ€**ã€‚

---

## é€‚ç”¨åœºæ™¯

Pararun å°¤å…¶é€‚åˆä»¥ä¸‹å·¥ä½œæµï¼š

* æ•°æ®å¤„ç† / æ¸…æ´— / è½¬æ¢è„šæœ¬
* æ‰¹é‡è°ƒç”¨ API / æ¨¡åž‹ / æŽ¨ç†æœåŠ¡
* é•¿æ—¶é—´è¿è¡Œã€**å¯èƒ½ä¸­é€”å¤±è´¥**çš„ä»»åŠ¡
* éœ€è¦**æ–­ç‚¹ç»­è·‘ï¼ˆå¹‚ç­‰ï¼‰**çš„å®žéªŒæˆ–ç¦»çº¿ä½œä¸š
* ä½¿ç”¨ Pythonï¼Œä½†ä¸æƒ³å¼•å…¥ Airflow / Ray / Celery è¿™ç±»é‡é‡çº§ç³»ç»Ÿ

ä¸€å¥è¯æ¦‚æ‹¬ï¼š

> **Pararun æ˜¯â€œå†™ç»™è„šæœ¬ä½œè€…çš„å¹¶å‘æ‰§è¡Œæ¡†æž¶â€ã€‚**

---

## æ ¸å¿ƒè®¾è®¡æ€æƒ³

### 1. å¹¶å‘ä¸æ˜¯ç›®çš„ï¼Œ**å¯æ¢å¤æ€§æ‰æ˜¯**

åœ¨çœŸå®žçŽ¯å¢ƒä¸­ï¼Œä»»åŠ¡å¤±è´¥å¾€å¾€ä¸æ˜¯å› ä¸ºé€»è¾‘é”™è¯¯ï¼Œè€Œæ˜¯å› ä¸ºï¼š

* æœºå™¨é‡å¯
* ç½‘ç»œæ³¢åŠ¨
* OOM / è¶…æ—¶
* äººä¸ºä¸­æ–­

Pararun æŠŠâ€œå¤±è´¥æ˜¯å¸¸æ€â€ä½œä¸ºå‰ææ¥è®¾è®¡ï¼š

* æ¯ä¸ªä»»åŠ¡ç»“æžœéƒ½ä¼š**æŒç»­å†™å…¥ JSONL æ–‡ä»¶**
* å†æ¬¡è¿è¡Œæ—¶ï¼Œè‡ªåŠ¨è·³è¿‡å·²å®Œæˆä»»åŠ¡
* **ä¸éœ€è¦é¢å¤–æ•°æ®åº“ï¼Œä¸éœ€è¦é¢å¤–é…ç½®**

è¿™ä¸æ˜¯ç¼“å­˜ä¼˜åŒ–ï¼Œè€Œæ˜¯**å¹‚ç­‰æ‰§è¡Œæ¨¡åž‹**ã€‚

---

### 2. ä¸€ä¸ªç»Ÿä¸€ APIï¼Œè¦†ç›–ä¸‰ç§å¹¶å‘æ¨¡åž‹

Pararun å¹¶ä¸å¼ºè¿«ä½ é€‰æŠ€æœ¯è·¯çº¿ï¼Œè€Œæ˜¯æ ¹æ®ä»»åŠ¡æ€§è´¨è‡ªç„¶åˆ†æµï¼š

* `pr.map`
  ç”¨äºŽ **åŒæ­¥ / é˜»å¡žå‡½æ•°**
  å†…éƒ¨åŸºäºŽ `concurrent.futures`ï¼ˆçº¿ç¨‹ / è¿›ç¨‹ï¼‰

* `pr.aio_map`
  ç”¨äºŽ **åŽŸç”Ÿ async / asyncio å‡½æ•°**
  ä¸ç ´å event loop è¯­ä¹‰

ä½ ä¸éœ€è¦ä¸ºâ€œå¹¶å‘æ–¹å¼â€é‡å†™ä¸šåŠ¡ä»£ç ã€‚

---

### 3. å¤©ç„¶æ”¯æŒâ€œæµå¼å¤§æ•°æ®é›†â€

Pararun ä»Žä¸€å¼€å§‹å°±å‡è®¾ï¼š

> ä½ çš„è¾“å…¥ï¼Œå¯èƒ½å¤§åˆ°**æ ¹æœ¬æ”¾ä¸è¿›å†…å­˜**ã€‚

å› æ­¤å®ƒæ”¯æŒï¼š

* ç›´æŽ¥ä¼ å…¥ **ç”Ÿæˆå™¨**
* å†…éƒ¨ä½¿ç”¨æœ‰ç•Œé˜Ÿåˆ— / ä¿¡å·é‡
* è‡ªåŠ¨ååŽ‹ï¼ˆbackpressureï¼‰
* å†…å­˜å ç”¨ä¸Ž `n_workers` æˆæ­£æ¯”

å³ä½¿ä½ å¤„ç†çš„æ˜¯åƒä¸‡çº§ä»»åŠ¡ï¼Œä¹Ÿä¸ä¼šâ€œåƒçˆ†å†…å­˜â€ã€‚

---

### 4. å†…å»ºè¿›åº¦ä¸Žå¯è§‚æµ‹æ€§

* åŽŸç”Ÿé›†æˆ `tqdm`
* å®žæ—¶æ˜¾ç¤ºå®Œæˆè¿›åº¦
* éžä¾µå…¥å¼ï¼Œä¸å½±å“åŽŸæœ‰è¿”å›žç»“æž„

ä½ ä¸éœ€è¦å†æ‰‹å†™ä¸€å †è¿›åº¦ç»Ÿè®¡é€»è¾‘ã€‚

---

## å¿«é€Ÿå¼€å§‹

### å¹¶å‘æ‰§è¡ŒåŒæ­¥ä»»åŠ¡ï¼ˆCPU / IOï¼‰

```python
import pararun as pr
import time

def process_file(filename):
    time.sleep(0.1)
    return {"id": filename, "status": "done"}

files = (f"data_{i}.txt" for i in range(100))

pr.map(
    func=process_file,
    iterable=files,
    n_workers=4,
    cache_path="results.jsonl"
)
```

* æ”¯æŒ list / generator
* ç»“æžœè‡ªåŠ¨å†™å…¥ `results.jsonl`
* ä¸­æ–­åŽå¯ç›´æŽ¥é‡è·‘

---

### å¹¶å‘æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡ï¼ˆAsyncIOï¼‰

```python
import pararun as pr
import asyncio

async def fetch_url(item):
    await asyncio.sleep(0.1)
    return {"id": item["url"], "status": 200}

async def main():
    urls = [{"url": f"https://example.com/{i}"} for i in range(100)]
    
    await pr.aio_map(
        func=fetch_url,
        iterable=urls,
        n_workers=10,
        cache_path="async_results.jsonl"
    )

asyncio.run(main())
```

---

## å¹‚ç­‰æ‰§è¡Œä¸Žæ–­ç‚¹ç»­è·‘

åªè¦æŒ‡å®š `cache_path`ï¼š

* Pararun ä¼šè‡ªåŠ¨è¯»å–å·²æœ‰ç»“æžœ
* æ ¹æ® `id`ï¼ˆæˆ–è‡ªå®šä¹‰å­—æ®µï¼‰åˆ¤æ–­æ˜¯å¦å·²å¤„ç†
* **å·²å®Œæˆä»»åŠ¡ä¸ä¼šå†æ¬¡æ‰§è¡Œ**

```python
pr.map(
    ...,
    cache_path="cache.jsonl",
    key_field="filename"
)
```

å…¸åž‹ä½¿ç”¨æ–¹å¼ï¼š

* ç¬¬ä¸€æ¬¡è·‘ä¸€åŠ â†’ ç¨‹åºä¸­æ–­
* ç¬¬äºŒæ¬¡ç›´æŽ¥é‡è·‘ â†’ è‡ªåŠ¨ä»Žæ–­ç‚¹ç»§ç»­

---

# Pararun

**Pararun** is a lightweight, fault-tolerant Python library for concurrent and parallel task execution. It simplifies running tasks using `asyncio`, `multiprocessing`, or `threading`, with built-in support for persistent caching (idempotency), progress bars, and streaming large datasets.

## Features

- ðŸš€ **Unified API**: Simple `pr.map` for parallel processing and `pr.aio_map` for async tasks.
- ðŸ’¾ **Idempotent Caching**: Automatically skips processed items by checking a JSONL cache file. Perfect for resumable long-running jobs.
- ðŸŒŠ **Streaming Support**: Handles large datasets (generators) with controlled memory usage using backpressure.
- ðŸ“Š **Progress Monitoring**: Integrated `tqdm` progress bars.
- ðŸ›¡ï¸ **Fault Tolerance**: Safely handles crashes by flushing results to disk periodically.

## Installation

```bash
pip install pararun
```

## Quick Start

### 1. Parallel Processing (CPU/IO Bound)

Use `pr.map` for blocking functions. It uses `concurrent.futures` implementation.

```python
import pararun as pr
import time

def process_file(filename):
    time.sleep(0.1)  # Simulate blocking work
    return {"id": filename, "status": "done"}

# Works with Lists or Generators
files = (f"data_{i}.txt" for i in range(100))

# Result is saved to 'results.jsonl' automatically
pr.map(
    func=process_file,
    iterable=files,
    n_workers=4,
    cache_path="results.jsonl"
)
```

### 2. Async Processing (AsyncIO)

Use `pr.aio_map` for native async functions.

```python
import pararun as pr
import asyncio

async def fetch_url(item):
    await asyncio.sleep(0.1) # Simulate network request
    return {"id": item["url"], "status": 200}

async def main():
    urls = [{"url": f"https://example.com/{i}"} for i in range(100)]
    
    await pr.aio_map(
        func=fetch_url,
        iterable=urls,
        n_workers=10,
        cache_path="async_results.jsonl"
    )

if __name__ == "__main__":
    asyncio.run(main())
```

## Advanced Usage

### Idempotency & Resuming
When `cache_path` is provided, `pararun` reads the file (if it exists) to verify which items have already been processed. 

By default, it assumes the output items contain an `"id"` field. You can customize this field using the `key_field` parameter:

```python
pr.map(..., cache_path="cache.jsonl", key_field="filename")
```

- **Run 1**: Process 50% of items -> Crash.
- **Run 2**: Point to same `cache_path`. `pararun` skips the first 50% and resumes from where it left off.

### Streaming Large Datasets
`pararun` is designed to be memory efficient. It uses bounded queues (semaphores) to ensure that even if you pass a generator with 100M items, only `n_workers * 2` items are held in memory at any time.

## Development

Install dependencies and run tests:

```bash
# Install package in editable mode
pip install -e .

# Run tests
python -m pytest
```

## License

MIT
